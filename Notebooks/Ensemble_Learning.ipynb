{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from nltk import word_tokenize\n",
    "from preprocessing import PreProcessor #this is our own preprocessor\n",
    "from sys import stdout\n",
    "import warnings \n",
    "import numpy as np\n",
    "import tqdm\n",
    "import scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter('ignore')\n",
    "preprocess = PreProcessor().preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidfngramsvecttwograms = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidfngramsvectthreegrams = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "def tfidf(corpus, ngrams):\n",
    "    if ngrams == 1:\n",
    "        return tfidfvect.fit_transform(corpus)\n",
    "    elif ngrams == 2:\n",
    "        return tfidfngramsvecttwograms.fit_transform(corpus)\n",
    "    else: return tfidfngramsvectthreegrams.fit_transform(corpus)\n",
    "\n",
    "def word2vec(corpus, size):\n",
    "    tokenized = [word_tokenize(row) for row in corpus]\n",
    "    model = Word2Vec(tokenized, size=size, workers=8)\n",
    "    vectors = []\n",
    "    for i, row in enumerate(tokenized):\n",
    "        sentence_vectors = [model.wv[word] for word in row if word in model.wv]\n",
    "        if len(sentence_vectors) == 0:\n",
    "            vectors.append([0] * size)\n",
    "        else:\n",
    "            sentence_vector = np.average(sentence_vectors, axis=0)\n",
    "            vectors.append(sentence_vector)\n",
    "    return vectors, model\n",
    "\n",
    "def fasttext(corpus, size):\n",
    "    tokenized = [word_tokenize(row) for row in corpus]\n",
    "    model = FastText(tokenized, size=size, workers=2)\n",
    "    vectors = []\n",
    "    for i, row in enumerate(tokenized):\n",
    "        sentence_vectors = [model.wv[word] for word in row]\n",
    "        if len(sentence_vectors) == 0:\n",
    "            vectors.append([0] * size)\n",
    "        else:\n",
    "            sentence_vector = np.average(sentence_vectors, axis=0)\n",
    "            vectors.append(sentence_vector)\n",
    "    return vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(data):\n",
    "    vectors = []\n",
    "    \n",
    "    print(\"Preprocessing for Linear SVC\")\n",
    "    MinimalStripping = data.apply(lambda d: preprocess(d , lower_case=False, strip=True, punctuation=False, numbers=False, unicode=False, cut_off=False, stop_words=False, stemming=False, lemmatizing=False, min_word_length=-1, max_word_length=-1, tokenize=False))\n",
    "    print(\"Done preprocessing, Vectorizing for Linear SVC\")\n",
    "    vectors.append(tfidf(MinimalStripping, 2))\n",
    "    \n",
    "    print(\"Preprocessing for Random Forest\")\n",
    "    DefaultStripping = data.apply(lambda d: preprocess(d, lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=True, stemming=False, lemmatizing=True, min_word_length=1, max_word_length=-1, tokenize=False))\n",
    "    print(\"Done preprocessing, Vectorizing for Random Forest\")\n",
    "    vectors.append(tfidf(DefaultStripping, 1))\n",
    "    \n",
    "    print(\"Preprocessing for Linear Discriminant\")\n",
    "    ExtremeStripping = data.apply(lambda d: preprocess(d, lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=True, stemming=False, lemmatizing=True, min_word_length=1, max_word_length=-1, tokenize=False))\n",
    "    print(\"Done preprocessing, Vectorizing for Linear Discriminant\")\n",
    "    fasttextvectors, fasttextmodel = fasttext(ExtremeStripping, 64)\n",
    "    vectors.append(fasttextvectors)\n",
    "    \n",
    "    print(\"Preprocessing for Decision Tree\")\n",
    "    DefaultStrippingStemming = data.apply(lambda d: preprocess(d, lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=False, stemming=True, lemmatizing=False, min_word_length=1, max_word_length=-1, tokenize=False))\n",
    "    print(\"Done preprocessing, Vectorizing for Decision Tree\")\n",
    "    w2vvectors, w2vmodel = word2vec(DefaultStrippingStemming, 256)\n",
    "    vectors.append(w2vvectors)\n",
    "    \n",
    "    print(\"No preprocessing needed for Logistic Regression, Vectorizing for Logistic Regression\")\n",
    "    vectors.append(tfidf(MinimalStripping, 3))\n",
    "    \n",
    "    print(\"No preprocessing needed for Multinomial Naive Bayes, Vectorizing for Multinomial Naive Bayes\")\n",
    "    w2vvectors2, w2vmodel2 = word2vec(DefaultStrippingStemming, 256)\n",
    "    vectors.append(w2vvectors2)\n",
    "    \n",
    "    return vectors, w2vmodel, w2vmodel2, fasttextmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization for new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessMinimalTwoGrams(sentence, model):\n",
    "    return tfidfngramsvecttwograms.transform([preprocess(sentence, lower_case=False, strip=True, punctuation=False, numbers=False, unicode=False, cut_off=False, stop_words=False, stemming=False, lemmatizing=False, min_word_length=-1, max_word_length=-1, tokenize=False)])\n",
    "\n",
    "def PreprocessMinimalThreeGrams(sentence, model):\n",
    "    return tfidfngramsvectthreegrams.transform([preprocess(sentence, lower_case=False, strip=True, punctuation=False, numbers=False, unicode=False, cut_off=False, stop_words=False, stemming=False, lemmatizing=False, min_word_length=-1, max_word_length=-1, tokenize=False)])\n",
    "\n",
    "def PreprocessDefault(sentence, model):\n",
    "    return tfidfvect.transform([preprocess(sentence, lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=True, stemming=False, lemmatizing=True, min_word_length=1, max_word_length=-1, tokenize=False)])\n",
    "\n",
    "def PreprocessExtremeStemming(sentence, model):\n",
    "    PExtremeStemming = preprocess(str(sentence), lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=True, stemming=False, lemmatizing=True, min_word_length=1, max_word_length=-1, tokenize=False)\n",
    "    return np.average([model.wv[word] for word in word_tokenize(PExtremeStemming) if word in model.wv], axis=0)\n",
    "\n",
    "def PreprocessDefaultStemming(sentence, model):\n",
    "    PDefaultStemming = preprocess(str(sentence), lower_case=True, strip=True, punctuation=True, numbers=True, unicode=True, cut_off=True, stop_words=False, stemming=True, lemmatizing=False, min_word_length=1, max_word_length=-1, tokenize=True)\n",
    "    vectors = [model.wv[word] for word in PDefaultStemming if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.random.uniform(low=-1, high=1, size=(256,))\n",
    "    else:\n",
    "        return np.average(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcAccuracyPerCategory(model, X, Y):\n",
    "    unique = Y.unique()\n",
    "    result = pd.DataFrame(unique, columns = ['Category']) \n",
    "    result['true'] = 0\n",
    "    result['false'] = 0\n",
    "    result['accuracy'] = 0\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    for index in range(len(y_pred)):\n",
    "        \n",
    "        i = result.index[result['Category'] == Y.values[index]].tolist()[0]\n",
    "        if Y.values[index] == y_pred[index]: \n",
    "             result.at[i,'true'] = result.at[i,'true']+1\n",
    "        else: \n",
    "             result.at[i,'false'] = result.at[i,'false']+1\n",
    "                \n",
    "    \n",
    "    for i, row in result.iterrows():\n",
    "        total = row['true'] + row['false']\n",
    "        result.at[i,'accuracy'] = row['true']/total * 100\n",
    "    return result\n",
    "        \n",
    "def TrainModels(dataframe, vectors, w2vmodel, w2vmodel2, fasttextmodel):\n",
    "    models = []    \n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[0], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0) \n",
    "    modelSVC = LinearSVC().fit(trainX, trainY)\n",
    "    accuracySVC = CalcAccuracyPerCategory(modelSVC, validateX, validateY)\n",
    "    models.append([modelSVC, accuracySVC, \"Linear SVC\", PreprocessMinimalTwoGrams, None])\n",
    "    print(\"Done training model Linear SVC\")\n",
    "    \n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[1], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "    modelRF = RandomForestClassifier().fit(trainX, trainY)\n",
    "    accuracyRF = CalcAccuracyPerCategory(modelRF, validateX, validateY)\n",
    "    models.append([modelRF, accuracyRF, \"Random Forest\", PreprocessDefault, None])\n",
    "    print(\"Done training model Random Forest\")\n",
    "    \n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[2], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "    modelLD = LinearDiscriminantAnalysis().fit(trainX, trainY)\n",
    "    accuracyLD = CalcAccuracyPerCategory(modelLD, validateX, validateY)\n",
    "    models.append([modelLD, accuracyLD, \"Linear Discriminant\", PreprocessExtremeStemming, fasttextmodel])\n",
    "    print(\"Done training model Linear Discriminant\")\n",
    "\n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[3], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)      \n",
    "    modelDT = DecisionTreeClassifier().fit(trainX, trainY)\n",
    "    accuracyDT = CalcAccuracyPerCategory(modelDT, validateX, validateY)\n",
    "    models.append([modelDT, accuracyDT, \"Decision Tree\", PreprocessDefaultStemming, w2vmodel])\n",
    "    print(\"Done training model Decision Tree\")\n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[4], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "    lrc = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "    lrc.fit(trainX, trainY)\n",
    "    accuracyLR = CalcAccuracyPerCategory(lrc, validateX, validateY)\n",
    "    models.append([lrc, accuracyLR, \"Logistic Regression\", PreprocessMinimalThreeGrams, None])\n",
    "    print(\"Done training model Logistic Regression\")\n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(vectors[5], dataframe['Category'], test_size=0.2, random_state=0)\n",
    "    trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(trainX, trainY)\n",
    "    accuracyNB = CalcAccuracyPerCategory(gnb, validateX, validateY)\n",
    "    models.append([gnb, accuracyNB, \"Gaussian Naive Bayes\", PreprocessDefaultStemming, w2vmodel])\n",
    "    print(\"Done training model Gaussian Naive Bayes\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(x, total):\n",
    "    return x/total * 100\n",
    "\n",
    "def Predict(models, sentence):\n",
    "    data = []\n",
    "    \n",
    "    for model in models:\n",
    "        preprocessedsentencevector = model[3](sentence, model[4])\n",
    "        try:\n",
    "            prediction = list(model[0].predict(preprocessedsentencevector))[0]\n",
    "        except:\n",
    "            prediction = list(model[0].predict([preprocessedsentencevector]))[0]\n",
    "        index = model[1].index[model[1]['Category'] == prediction].tolist()[0]\n",
    "        accuracy = model[1].at[index,'accuracy']\n",
    "        data.append([prediction, accuracy])\n",
    "        \n",
    "    result = pd.DataFrame(data, columns = [ 'Category', 'Accuracy']) \n",
    "    result = result.groupby(['Category'], as_index=False).sum()\n",
    "    totalaccuracy = result['Accuracy'].sum()\n",
    "    \n",
    "    result['Accuracy'] = result['Accuracy'].apply(lambda d: percentage(d, totalaccuracy))\n",
    "    return result.sort_values(by=['Accuracy'], ascending=False).values[0][0]\n",
    "\n",
    "def PredictMultiple(models, sentences):\n",
    "    print(\"Testing multiple models\")\n",
    "    y_pred = []\n",
    "    time.sleep(1)\n",
    "    failedrecords = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        try:\n",
    "            y_pred.append(Predict(models, sentence))\n",
    "        except:\n",
    "            y_pred.append(\"NaN\")\n",
    "            for model in models:\n",
    "                failedrecords.append([sentence, model[3](sentence, model[4])])\n",
    "    return y_pred, failedrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where the magic happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for Linear SVC\n",
      "Done preprocessing, Vectorizing for Linear SVC\n",
      "Preprocessing for Random Forest\n",
      "Done preprocessing, Vectorizing for Random Forest\n",
      "Preprocessing for Linear Discriminant\n",
      "Done preprocessing, Vectorizing for Linear Discriminant\n",
      "Preprocessing for Decision Tree\n",
      "Done preprocessing, Vectorizing for Decision Tree\n",
      "No preprocessing needed for Logistic Regression, Vectorizing for Logistic Regression\n",
      "No preprocessing needed for Multinomial Naive Bayes, Vectorizing for Multinomial Naive Bayes\n",
      "Done training model Linear SVC\n",
      "Done training model Random Forest\n",
      "Done training model Linear Discriminant\n",
      "Done training model Decision Tree\n",
      "Done training model Logistic Regression\n",
      "Done training model Gaussian Naive Bayes\n",
      "Testing multiple models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22115/22115 [23:21<00:00, 15.78it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/Mapped.csv')\n",
    "df.columns = [\"Vendor\", \"Category\", \"Item\", \"Item Description\", \"Price\", \"Origin\", 'Destination', \"Rating\", \"Remarks\", \"randomshit\"]\n",
    "df['Category'] = df[\"Category\"].apply(lambda d: d.split('/', 1)[0])\n",
    "vectors, w2vmodel, w2vmodel2, fasttextmodel = Vectorize(df[\"Item\"] + \" \" + df['Item Description'])\n",
    "models = TrainModels(df, vectors, w2vmodel, w2vmodel2, fasttextmodel)\n",
    "\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(df[\"Item\"] + \" \" + df['Item Description'], df['Category'], test_size=0.2, random_state=0)\n",
    "trainX, validateX, trainY, validateY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "y_pred, failed = PredictMultiple(models, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9494912955007914, 0.9494912955007914, 0.9494912955007914, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(testY.values, y_pred, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
